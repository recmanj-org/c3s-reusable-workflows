name: Notebook QA

on:
  workflow_call:
    inputs:
      notebooks:
        description: 'Comma-separated list of notebook paths to check (e.g., ./notebook1.ipynb,./folder/notebook2.ipynb). Leave empty to check all notebooks.'
        required: false
        type: string
        default: ''
      pr-number:
        description: 'Pull request number (for Jira issue tracking)'
        required: false
        type: string
      pr-title:
        description: 'Pull request title (for Jira issue summary)'
        required: false
        type: string
      pr-url:
        description: 'Pull request URL (for Jira issue description)'
        required: false
        type: string
    secrets:
      CDSAPI_KEY:
        description: 'CDS API key for notebook execution'
        required: false
      JIRA_API_TOKEN:
        description: 'Jira API token for authentication'
        required: false
      JIRA_HOST:
        description: 'Jira instance URL (e.g., https://your-domain.atlassian.net)'
        required: false
      JIRA_PROJECT_KEY:
        description: 'Jira project key (e.g., CDSQA)'
        required: false

jobs:
  checks:
    strategy:
      matrix:
        os: [ubuntu-24.04]
        python-version: [3.13]
      fail-fast: false
    runs-on: ${{ matrix.os }}
    defaults:
      run:
        shell: bash -l {0}
    env:
      QA_OUTPUT_DIR: qa_outputs${{ github.run_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Conda environment
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniforge-version: "latest"
          environment-file: "environment.yml"
          activate-environment: notebook-qa
          auto-activate-base: false
          use-mamba: true
          channels: conda-forge
          use-only-tar-bz2: true
          python-version: ${{ matrix.python-version }}

      - name: Install QA dependencies
        run: |
          conda install pip
          # Pin click to <8.3 to avoid issues with pynblint bool flags
          pip install nbqa ruff pynblint 'click<8.3' pytest-check-links ploomber-engine psutil matplotlib \
                      beautifulsoup4 lxml pytest pytest-cov

          if command -v apt-get &> /dev/null; then
            echo "Installing jq via apt-get..."
            sudo apt-get -y update && sudo apt-get -y --no-install-recommends install jq
          elif command -v brew &> /dev/null; then
            echo "Installing jq via brew..."
            brew install jq
          elif command -v choco &> /dev/null; then
            echo "Installing jq via choco..."
            choco install jq
          else
            echo "Error: No package manager found (apt-get, brew, or choco)"
            exit 1
          fi

      - name: Collect notebooks
        id: collect_notebooks
        shell: python
        env:
          MANUAL_NOTEBOOKS: ${{ inputs.notebooks }}
        run: |
          import json
          import os
          import sys

          manual_notebooks = os.environ.get('MANUAL_NOTEBOOKS', '').strip()

          if manual_notebooks:
              # Parse comma-separated notebook paths
              notebooks = [nb.strip() for nb in manual_notebooks.split(',') if nb.strip()]

              # Validate that all specified notebooks exist
              missing_notebooks = []
              for notebook in notebooks:
                  if not os.path.isfile(notebook):
                      missing_notebooks.append(notebook)

              if missing_notebooks:
                  print(f"Error: The following notebooks do not exist:", file=sys.stderr)
                  for nb in missing_notebooks:
                      print(f"  - {nb}", file=sys.stderr)
                  sys.exit(1)

              print(f"Using manually specified notebooks: {notebooks}")
          else:
              # Default behavior: find all notebooks
              notebooks = []
              for root, dirs, files in os.walk('.'):
                  for filename in files:
                      if filename.endswith('.ipynb'):
                          notebook_path = os.path.join(root, filename)
                          notebooks.append(notebook_path)

              print(f"Found {len(notebooks)} notebooks in repository")

          notebooks_json = json.dumps(notebooks)
          with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
              fh.write(f"notebooks={notebooks_json}\n")

      - name: (2.2.3) Run linter
        id: linter
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: linter
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (2.2.3) Run formatter
        id: formatter
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: formatter
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (2.2.3) Run pynblint
        id: pynblint
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: pynblint
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}
          pynblint_include_violations: 'non-linear-execution,untitled-notebook,on-portable-chars-in-nb-name,missing-h1-MD-heading,missing-opening-MD-text,missing-closing-MD-text,too-few-MD-cells,duplicate-notebook-not-renamed,non-executed-notebook,non-executed-cells,empty-cells'

      - name: (1.2.5) Check for DOI references
        id: doi_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: doi_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (1.2.3) Run link checker
        id: link_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: link_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (2.2.1, 2.2.4, 2.2.6) Execute notebooks
        id: execute_notebooks
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        env:
          CDSAPI_KEY: ${{ secrets.CDSAPI_KEY || '' }}
        with:
          command: execute
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (1.2.6) Check version metadata
        id: metadata_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: metadata_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (2.3.1, 2.3.2) Check tests and coverage
        id: test_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: test_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}
          coverage_threshold: '80'

      - name: (3.1.3) Check accessibility - alt text
        id: accessibility_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: accessibility_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: Upload notebook outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: notebook-outputs-${{ github.run_id }}
          path: ${{ env.QA_OUTPUT_DIR }}
          if-no-files-found: warn

      - name: (1.2.4) Check LICENSE
        id: license
        if: success() || failure()
        run: |
          if [[ ! -f "LICENSE" ]]; then
            echo "LICENSE file is missing"
            exit 1
          else
            echo "LICENSE file is present"
          fi
          if [ ! -s "LICENSE" ]; then
            echo "LICENSE file is empty"
            exit 1
          else
            echo "LICENSE file is not empty"
          fi

      - name: (4.2.3) Check CHANGELOG
        id: changelog
        if: success() || failure()
        run: |
          if [[ ! -f "CHANGELOG.md" ]]; then
            echo "CHANGELOG.md file is missing"
            exit 1
          else
            echo "CHANGELOG.md file is present"
          fi
          if [ ! -s "CHANGELOG.md" ]; then
            echo "CHANGELOG.md file is empty"
            exit 1
          else
            echo "CHANGELOG.md file is not empty"
          fi

      - name: Generate Workflow Summary
        if: always()
        shell: python
        run: |
          import csv
          import json
          import os
          from pathlib import Path

          # Get step outcomes
          outcomes = {
              '(1.2.3) Link Checker': '${{ steps.link_checker.outcome }}',
              '(1.2.4) License Check': '${{ steps.license.outcome }}',
              '(1.2.5) DOI Checker': '${{ steps.doi_checker.outcome }}',
              '(1.2.6) Metadata Check': '${{ steps.metadata_checker.outcome }}',
              '(2.2.3) Linter': '${{ steps.linter.outcome }}',
              '(2.2.3) Formatter': '${{ steps.formatter.outcome }}',
              '(2.2.3) Pynblint': '${{ steps.pynblint.outcome }}',
              '(2.2.1, 2.2.4, 2.2.6) Execute Notebooks': '${{ steps.execute_notebooks.outcome }}',
              '(2.3.1, 2.3.2) Test & Coverage': '${{ steps.test_checker.outcome }}',
              '(3.1.3) Accessibility': '${{ steps.accessibility_checker.outcome }}',
              '(4.2.3) Changelog Check': '${{ steps.changelog.outcome }}',  
          }

          # Map outcomes to status icons
          status_map = {
              'success': '‚úÖ Pass',
              'failure': '‚ùå Fail',
              'skipped': '‚è≠Ô∏è Skipped',
              'cancelled': 'üö´ Cancelled',
              '': '‚è≠Ô∏è Skipped'
          }

          # Map individual notebook statuses to icons
          notebook_status_map = {
              'success': '‚úÖ',
              'failure': '‚ùå',
              'skipped': '‚è≠Ô∏è',
          }

          summary = []
          summary.append('# Notebook QA Summary\n')

          # Add overall status table
          summary.append('## Overall Check Results\n')
          summary.append('| Check | Status |')
          summary.append('|-------|--------|')

          for check_name, outcome in outcomes.items():
              status = status_map.get(outcome, outcome)
              summary.append(f'| {check_name} | {status} |')

          summary.append('\n')

          # Read per-notebook results
          qa_output_dir = os.getenv('QA_OUTPUT_DIR', 'qa_outputs')
          output_path = Path(qa_output_dir)

          # Command mapping for table headers
          command_headers = {
              'link_checker': 'Links',
              'doi_checker': 'DOI',
              'metadata_checker': 'Metadata',
              'linter': 'Linter',
              'formatter': 'Formatter',
              'pynblint': 'Pynblint',
              'execute': 'Execute',
              'test_checker': 'Tests',
              'accessibility_checker': 'Alt-text'
          }

          # Collect all notebook results
          all_notebooks = set()
          results_by_command = {}

          if output_path.exists():
              # Read all result JSON files
              for result_file in output_path.glob('*-results.json'):
                  try:
                      with open(result_file, 'r') as f:
                          data = json.load(f)
                          command = data.get('command', '')
                          results = data.get('results', {})
                          results_by_command[command] = results
                          all_notebooks.update(results.keys())
                  except Exception as e:
                      print(f"Warning: Failed to read {result_file}: {e}")

          # Generate per-notebook results table if we have results
          if all_notebooks and results_by_command:
              summary.append('## Per-Notebook Results\n')

              # Create table header
              header = '| Notebook |'
              separator = '|----------|'
              for cmd in ['linter', 'formatter', 'pynblint', 'doi_checker', 'link_checker', 'execute',
                          'metadata_checker', 'test_checker', 'accessibility_checker']:
                  if cmd in results_by_command:
                      header += f' {command_headers[cmd]} |'
                      separator += '--------|'

              summary.append(header)
              summary.append(separator)

              # Create table rows for each notebook
              for notebook in sorted(all_notebooks):
                  row = f'| {notebook} |'

                  for cmd in ['linter', 'formatter', 'pynblint', 'doi_checker', 'link_checker', 'execute',
                              'metadata_checker', 'test_checker', 'accessibility_checker']:
                      if cmd in results_by_command:
                          status = results_by_command[cmd].get(notebook, 'skipped')
                          icon = notebook_status_map.get(status, '‚è≠Ô∏è')
                          row += f' {icon} |'

                  summary.append(row)

              summary.append('\n')

          # Add memory profiling section

          if output_path.exists():
              # Add CSV memory profiling data tables
              profiling_csvs = sorted(output_path.glob('*-profiling-data.csv'))

              if profiling_csvs:
                  for csv_path in profiling_csvs:
                      # Extract notebook name from CSV filename
                      # Format: <notebook-name>.output-profiling-data.csv
                      notebook_name = csv_path.stem.replace('.output-profiling-data', '')

                      # Parse CSV file
                      try:
                          with open(csv_path, 'r') as csvfile:
                              reader = csv.DictReader(csvfile)
                              rows = list(reader)

                          if rows:
                              # Create collapsible section for each notebook
                              summary.append(f'\n<details>\n<summary><b>{notebook_name}</b> ({len(rows)} cells)</summary>\n\n')

                              # Create table header
                              summary.append('| Cell | Memory (MB) | Time (s) |')
                              summary.append('|------|-------------|----------|')

                              # Add rows for each cell
                              for row in rows:
                                  cell = row.get('cell', 'N/A')
                                  memory_mb = row.get('memory', 'N/A')
                                  if memory_mb != 'N/A':
                                      try:
                                          memory_mb = f"{float(memory_mb):.2f}"
                                      except ValueError:
                                          pass
                                  exec_time_s = row.get('runtime', 'N/A')
                                  if exec_time_s != 'N/A':
                                      try:
                                          exec_time_s = f"{float(exec_time_s):.2f}"
                                      except ValueError:
                                          pass

                                  summary.append(f'| {cell} | {memory_mb} | {exec_time_s} |')

                              summary.append('\n</details>\n')
                      except Exception as e:
                          print(f'Warning: Failed to parse {csv_path}: {e}')
                          summary.append(f'\n*Could not parse profiling data for {notebook_name}*\n')

              summary.append('\n**View the generated memory profile charts by downloading the workflow artifacts.**\n')

          summary_text = '\n'.join(summary)
          github_summary = os.getenv('GITHUB_STEP_SUMMARY')
          with open(github_summary, 'a') as f:
              f.write(summary_text)

      - name: Install Jira CLI
        if: always() && inputs.pr-number != '' && secrets.JIRA_API_TOKEN != '' && secrets.JIRA_HOST != '' && secrets.JIRA_PROJECT_KEY != ''
        run: |
          echo "Installing Jira CLI v1.7.0..."

          wget -q https://github.com/ankitpokhrel/jira-cli/releases/download/v1.7.0/jira_1.7.0_linux_x86_64.tar.gz

          tar -xzf jira_1.7.0_linux_x86_64.tar.gz
          sudo mv bin/jira /usr/local/bin/
          sudo chmod +x /usr/local/bin/jira

          jira version

          echo "Jira CLI installed successfully"

      - name: Configure Jira CLI
        if: always() && inputs.pr-number != '' && secrets.JIRA_API_TOKEN != '' && secrets.JIRA_HOST != '' && secrets.JIRA_PROJECT_KEY != ''
        run: |
          echo "Configuring Jira CLI..."

          cat > ~/.jira.yml << EOF
          installation: Cloud
          project:
            key: ${{ secrets.JIRA_PROJECT_KEY }}
          server: ${{ secrets.JIRA_HOST }}
          auth_type: bearer
          EOF

          echo "Jira CLI configured successfully"

      - name: Create or Update Jira Issue
        if: always() && inputs.pr-number != '' && secrets.JIRA_API_TOKEN != '' && secrets.JIRA_HOST != '' && secrets.JIRA_PROJECT_KEY != ''
        env:
          JIRA_API_TOKEN: ${{ secrets.JIRA_API_TOKEN }}
        run: |
          echo "Processing Jira issue for ${{ github.repository }} PR #${{ inputs.pr-number }}..."

          QA_OUTPUT_DIR="${{ env.QA_OUTPUT_DIR }}"
          HAS_FAILURES=false
          FAILED_CHECKS=""

          if [ -d "$QA_OUTPUT_DIR" ]; then
            for result_file in "$QA_OUTPUT_DIR"/*-results.json; do
              if [ -f "$result_file" ]; then
                command_name=$(jq -r '.command // "unknown"' "$result_file")

                failures=$(jq -r '.results | to_entries[] | select(.value == "failure") | .key' "$result_file")

                if [ -n "$failures" ]; then
                  HAS_FAILURES=true
                  failure_count=$(echo "$failures" | wc -l)
                  FAILED_CHECKS="${FAILED_CHECKS}- ${command_name}: ${failure_count} notebook(s) failed\n"
                fi
              fi
            done
          fi

          WORKFLOW_RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          CURRENT_DATE=$(date -u +"%Y-%m-%d %H:%M:%S UTC")

          JQL="project = ${{ secrets.JIRA_PROJECT_KEY }} AND summary ~ \"${{ github.repository }} PR #${{ inputs.pr-number }}\" AND statusCategory != Done"
          echo "Searching for existing issue with JQL: $JQL"

          EXISTING_ISSUE=$(jira issue list --jql "$JQL" --plain --no-headers --columns KEY 2>/dev/null | head -n 1 || true)

          if [ "$HAS_FAILURES" = true ]; then
            COMMENT_BODY="‚ùå **Workflow Run Failed**

**Run**: ${WORKFLOW_RUN_URL}
**Date**: ${CURRENT_DATE}

**Failed Checks**:
$(echo -e "$FAILED_CHECKS")

**Details**: See workflow run for complete logs."

            if [ -n "$EXISTING_ISSUE" ]; then
              echo "Updating existing issue: $EXISTING_ISSUE"
              jira issue comment add "$EXISTING_ISSUE" "$COMMENT_BODY"
              echo "Comment added to issue $EXISTING_ISSUE"
            else
              echo "Creating new Jira issue..."

              ISSUE_SUMMARY="Notebook QA Failed - ${{ github.repository }} PR #${{ inputs.pr-number }}: ${{ inputs.pr-title }}"
              ISSUE_DESCRIPTION="Automated issue created by notebook-qa workflow.

**Workflow Run**: ${WORKFLOW_RUN_URL}
**Pull Request**: ${{ inputs.pr-url }}
**Repository**: ${{ github.repository }}
**Branch**: ${{ github.ref_name }}"

              NEW_ISSUE=$(jira issue create \
                --type Bug \
                --summary "$ISSUE_SUMMARY" \
                --body "$ISSUE_DESCRIPTION" \
                --priority Medium \
                --label notebook-qa \
                --label automated \
                --label github-actions \
                --plain --no-input 2>&1 | grep -oE '[A-Z]+-[0-9]+' | head -n 1 || true)

              if [ -n "$NEW_ISSUE" ]; then
                echo "Created Jira issue: $NEW_ISSUE"

                jira issue comment add "$NEW_ISSUE" "$COMMENT_BODY"
                echo "Added failure details to $NEW_ISSUE"
              else
                echo "Warning: Failed to create Jira issue"
              fi
            fi
          else
            SUCCESS_COMMENT="‚úÖ **Workflow Run Succeeded**

**Run**: ${WORKFLOW_RUN_URL}
**Date**: ${CURRENT_DATE}

All checks passed successfully."

            if [ -n "$EXISTING_ISSUE" ]; then
              echo "Workflow passed. Adding success comment to existing issue: $EXISTING_ISSUE"
              jira issue comment add "$EXISTING_ISSUE" "$SUCCESS_COMMENT"
              echo "Success comment added to issue $EXISTING_ISSUE"
            else
              echo "Workflow passed and no existing issue found. No action needed."
            fi
          fi

          echo "Jira issue processing completed"
