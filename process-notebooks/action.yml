name: 'Process Notebooks'
description: 'Run QA tools on Jupyter notebooks cross-platform'
inputs:
  command:
    description: 'Command to run (linter, formatter, pynblint, link_checker, doi_checker, execute, metadata_checker, test_checker, accessibility_checker, figure_checker)'
    required: true
  notebooks:
    description: 'JSON array of notebook paths'
    required: true
  pynblint_include_violations:
    description: 'List of violations to include when running pynblint (comma-separated)'
    required: false
    default: ''
  coverage_threshold:
    description: 'Minimum code coverage percentage for test_checker (default: 80)'
    required: false
    default: '80'
runs:
  using: 'composite'
  steps:
    - name: Process notebooks
      shell: bash -l {0}
      run: |
        python3 - << 'EOF'
        import json
        import os
        import re
        import subprocess
        import sys
        import xml.etree.ElementTree as ET

        from pathlib import Path
        from ploomber_engine import execute_notebook

        try:
            from bs4 import BeautifulSoup
        except ImportError:
            BeautifulSoup = None

        command = "${{ inputs.command }}"
        notebooks_json = """${{ inputs.notebooks }}""".strip()

        valid_commands = ['linter', 'formatter', 'pynblint', 'link_checker', 'doi_checker', 'execute',
                          'metadata_checker', 'test_checker', 'accessibility_checker', 'figure_checker']
        if command not in valid_commands:
            print(f"Error: Invalid command '{command}'")
            print(f"Valid commands: {', '.join(valid_commands)}")
            sys.exit(1)

        if not notebooks_json:
            notebooks = []
        else:
            try:
                notebooks = json.loads(notebooks_json)
            except json.JSONDecodeError:
                print(f"Error: Invalid JSON format")
                sys.exit(1)

        result = 0
        notebook_results = {}

        for notebook in notebooks:
            if not notebook:
                continue

            print(f"Processing {notebook} with {command}")

            if command == "linter":
                cmd = ["nbqa", "ruff", "check", notebook]
            elif command == "formatter":
                cmd = ["nbqa", "ruff", "format", "--diff", notebook]
            elif command == "pynblint":
                notebook_name = Path(notebook).name
                lint_file = f"lint.{notebook_name}.json"
                cmd = ["pynblint", "--output", lint_file, notebook]
                include_violations_str = "${{ inputs.pynblint_include_violations }}".strip()
                if include_violations_str:
                    include_violations = [v.strip() for v in include_violations_str.split(',') if v.strip()]
                    if include_violations:
                        violations_json = '[' + ','.join(f'"{v}"' for v in include_violations) + ']'
                        cmd.extend(["--include", violations_json])
            elif command == "link_checker":
                cmd = ["pytest-check-links", notebook]
            elif command == "doi_checker":
                # Check for DOI in notebook
                # DOI regex patterns
                doi_patterns = [
                    r'10\.\d{4,9}/[-._;()/:A-Z0-9]+',  # Standard DOI format
                    r'doi\.org/(10\.\d{4,9}/[-._;()/:A-Z0-9]+)',  # doi.org URLs
                    r'https?://(?:dx\.)?doi\.org/(10\.\d{4,9}/[-._;()/:A-Z0-9]+)'  # Full DOI URLs
                ]

                # Metadata fields that might contain DOI references
                metadata_fields = ['references', 'citation', 'doi', 'reference', 'Attributes']

                # Read and parse notebook
                try:
                    with open(notebook, 'r', encoding='utf-8') as f:
                        nb_data = json.load(f)
                except Exception as e:
                    print(f"❌ Error reading notebook {notebook}: {e}")
                    notebook_results[notebook] = "failure"
                    result = 1
                    continue

                # First, check if dataset metadata contains DOI references
                has_dataset_doi_metadata = False

                for cell in nb_data.get('cells', []):
                    if cell.get('cell_type') == 'code':
                        for output in cell.get('outputs', []):
                            # Check text outputs for dataset metadata with DOIs
                            if 'text' in output:
                                text = output['text']
                                if isinstance(text, list):
                                    text = ''.join(text)
                                # Check if this looks like xarray/dataset output with metadata
                                if any(field in text for field in metadata_fields):
                                    # Check if metadata contains DOI patterns
                                    for pattern in doi_patterns:
                                        if re.search(pattern, text, re.IGNORECASE):
                                            has_dataset_doi_metadata = True
                                            break

                            # Check data outputs for metadata
                            data = output.get('data', {})
                            for key, value in data.items():
                                if isinstance(value, (str, list)):
                                    if isinstance(value, list):
                                        value = ''.join(value)
                                    if any(field in value for field in metadata_fields):
                                        for pattern in doi_patterns:
                                            if re.search(pattern, value, re.IGNORECASE):
                                                has_dataset_doi_metadata = True
                                                break

                            if has_dataset_doi_metadata:
                                break

                    if has_dataset_doi_metadata:
                        break

                # If no dataset metadata with DOIs found, skip the check
                if not has_dataset_doi_metadata:
                    print(f"INFO: No dataset DOI metadata found in {notebook}, skipping DOI check")
                    notebook_results[notebook] = "skipped"
                    continue

                # Dataset has DOI metadata, so we require DOI citations
                found_dois = set()

                # Search in all cells for DOI citations
                for cell in nb_data.get('cells', []):
                    # Check markdown cells
                    if cell.get('cell_type') == 'markdown':
                        source = cell.get('source', [])
                        if isinstance(source, list):
                            source = ''.join(source)
                        for pattern in doi_patterns:
                            matches = re.findall(pattern, source, re.IGNORECASE)
                            found_dois.update(matches)

                    # Check code cell outputs
                    if cell.get('cell_type') == 'code':
                        for output in cell.get('outputs', []):
                            # Check text outputs
                            if 'text' in output:
                                text = output['text']
                                if isinstance(text, list):
                                    text = ''.join(text)
                                for pattern in doi_patterns:
                                    matches = re.findall(pattern, text, re.IGNORECASE)
                                    found_dois.update(matches)

                            # Check data outputs (HTML, plain text, etc.)
                            data = output.get('data', {})
                            for key, value in data.items():
                                if isinstance(value, (str, list)):
                                    if isinstance(value, list):
                                        value = ''.join(value)
                                    for pattern in doi_patterns:
                                        matches = re.findall(pattern, value, re.IGNORECASE)
                                        found_dois.update(matches)

                # Validate DOI format
                valid_doi_pattern = re.compile(r'^10\.\d{4,9}/[-._;()/:A-Z0-9]+$', re.IGNORECASE)
                valid_dois = [doi for doi in found_dois if valid_doi_pattern.match(doi)]

                if not valid_dois:
                    print(f"ERROR: No valid DOI found in {notebook}")
                    print(f"  Notebook uses dataset with DOI metadata but doesn't cite the DOI")
                    print(f"  Please add DOI citation in markdown or ensure dataset metadata is visible")
                    print(f"  Expected format: 10.xxxx/xxxxx")
                    notebook_results[notebook] = "failure"
                    result = 1
                else:
                    print(f"Found {len(valid_dois)} valid DOI(s) in {notebook}:")
                    for doi in sorted(valid_dois):
                        print(f"  - {doi}")
                    notebook_results[notebook] = "success"

                # Skip subprocess call for doi_checker
                continue
            elif command == "execute":
                # Create cdsapi config file if needed
                cdsapi_key = os.getenv("CDSAPI_KEY")
                if cdsapi_key:
                    print("Creating .cdsapirc file")
                    cdsapi_rc = Path.home() / ".cdsapirc"
                    if not cdsapi_rc.exists():
                        with open(cdsapi_rc, 'w') as f:
                            f.write("url: https://cds.climate.copernicus.eu/api\n")
                            f.write(f"key: {cdsapi_key}")

                # Setup output directory and path
                output_dir = os.getenv("QA_OUTPUT_DIR", "qa_outputs")
                Path(output_dir).mkdir(parents=True, exist_ok=True)

                # Create output path with .output suffix
                notebook_path = Path(notebook)
                output_name = notebook_path.stem + ".output" + notebook_path.suffix
                output_path = Path(output_dir) / output_name

                # Execute notebook with memory profiling and CSV output
                try:
                    execute_notebook(
                        input_path=notebook,
                        output_path=str(output_path),
                        profile_memory=True,
                        save_profiling_data=True
                    )
                    print(f"✅ {command} finished successfully for {notebook}")
                    notebook_results[notebook] = "success"
                except Exception as e:
                    print(f"❌ {command} failed for {notebook}: {e}")
                    notebook_results[notebook] = "failure"
                    result = 1
                # Skip subprocess call for execute
                continue
            elif command == "metadata_checker":
                # Check for version date information in notebook
                try:
                    with open(notebook, 'r', encoding='utf-8') as f:
                        nb_data = json.load(f)
                except Exception as e:
                    print(f"❌ Error reading notebook {notebook}: {e}")
                    notebook_results[notebook] = "failure"
                    result = 1
                    continue

                # Date patterns to search for
                date_patterns = [
                    r'last\s+updated:?\s*(\d{4}-\d{2}-\d{2})',
                    r'version:?\s*[\d.]+\s*\((\d{4}-\d{2}-\d{2})\)',
                    r'modified:?\s*(\d{4}-\d{2}-\d{2})',
                    r'date:?\s*(\d{4}-\d{2}-\d{2})',
                    r'updated:?\s*(\d{4}-\d{2}-\d{2})',
                ]

                found_date = None
                found_location = None

                # Check notebook metadata
                metadata = nb_data.get('metadata', {})
                for field in ['date', 'modified', 'version', 'last_updated']:
                    if field in metadata:
                        value = str(metadata[field])
                        # Try to extract date from the metadata field
                        for pattern in date_patterns:
                            match = re.search(pattern, value, re.IGNORECASE)
                            if match:
                                found_date = match.group(1)
                                found_location = f"metadata.{field}"
                                break
                        if found_date:
                            break

                # Check markdown cells if not found in metadata
                if not found_date:
                    for cell_idx, cell in enumerate(nb_data.get('cells', [])):
                        if cell.get('cell_type') == 'markdown':
                            source = cell.get('source', [])
                            if isinstance(source, list):
                                source = ''.join(source)

                            # Search for date patterns
                            for pattern in date_patterns:
                                match = re.search(pattern, source, re.IGNORECASE)
                                if match:
                                    found_date = match.group(1)
                                    found_location = f"markdown cell {cell_idx}"
                                    break

                            if found_date:
                                break

                # Check for README.md if not found in notebook
                if not found_date:
                    readme_path = Path(notebook).parent / "README.md"
                    if readme_path.exists():
                        try:
                            with open(readme_path, 'r', encoding='utf-8') as f:
                                readme_content = f.read()

                            for pattern in date_patterns:
                                match = re.search(pattern, readme_content, re.IGNORECASE)
                                if match:
                                    found_date = match.group(1)
                                    found_location = "README.md"
                                    break
                        except Exception:
                            pass

                # Report results
                if found_date:
                    print(f"✅ Found version date for {notebook}: {found_date} (in {found_location})")
                    notebook_results[notebook] = "success"
                else:
                    print(f"❌ No version date found for {notebook}")
                    print(f"   Add 'Last updated: YYYY-MM-DD' to notebook markdown or README.md")
                    notebook_results[notebook] = "failure"
                    result = 1

                # Skip subprocess call for metadata_checker
                continue
            elif command == "test_checker":
                # Check for test files and run coverage analysis
                # This command checks both criteria 2.3.1 (test existence) and 2.3.2 (coverage)

                # PART 1: Check for test files (Criterion 2.3.1)
                test_patterns = ['test_*.py', '*_test.py', 'tests/**/*.py']
                test_files = []

                for pattern in test_patterns:
                    test_files.extend(Path('.').glob(pattern))

                if not test_files:
                    print(f"❌ No test files found for {notebook}")
                    print(f"   Create test_*.py or tests/ directory with test files")
                    notebook_results[notebook] = "failure"
                    result = 1
                    continue

                print(f"✅ Found {len(test_files)} test file(s)")
                for test_file in test_files:
                    print(f"   - {test_file}")

                # PART 2: Run tests with coverage (Criterion 2.3.2)
                coverage_threshold = 80.0  # Default threshold
                coverage_threshold_str = "${{ inputs.coverage_threshold }}".strip()
                if coverage_threshold_str:
                    try:
                        coverage_threshold = float(coverage_threshold_str)
                    except ValueError:
                        print(f"Warning: Invalid coverage threshold '{coverage_threshold_str}', using default 80%")

                print(f"\nRunning tests with coverage (threshold: {coverage_threshold}%)...")

                # Run pytest with coverage
                try:
                    pytest_result = subprocess.run(
                        ['pytest', '--cov=.', '--cov-report=xml', '--cov-report=term'],
                        capture_output=True,
                        text=True,
                        timeout=300  # 5 minute timeout
                    )

                    print(pytest_result.stdout)
                    if pytest_result.stderr:
                        print(pytest_result.stderr)

                    # Parse coverage.xml
                    coverage_xml = Path('coverage.xml')
                    if not coverage_xml.exists():
                        print(f"❌ Coverage report not generated")
                        notebook_results[notebook] = "failure"
                        result = 1
                        continue

                    tree = ET.parse('coverage.xml')
                    root = tree.getroot()
                    coverage_pct = float(root.attrib.get('line-rate', 0)) * 100

                    print(f"\nCode coverage: {coverage_pct:.1f}%")

                    if coverage_pct < coverage_threshold:
                        print(f"❌ Coverage {coverage_pct:.1f}% below threshold {coverage_threshold}%")
                        notebook_results[notebook] = "failure"
                        result = 1
                    else:
                        print(f"✅ Coverage {coverage_pct:.1f}% meets threshold {coverage_threshold}%")
                        notebook_results[notebook] = "success"

                except subprocess.TimeoutExpired:
                    print(f"❌ Tests timed out after 5 minutes")
                    notebook_results[notebook] = "failure"
                    result = 1
                except FileNotFoundError:
                    print(f"❌ pytest not found. Install pytest and pytest-cov")
                    notebook_results[notebook] = "failure"
                    result = 1
                except Exception as e:
                    print(f"❌ Error running tests: {e}")
                    notebook_results[notebook] = "failure"
                    result = 1

                # Skip subprocess call for test_checker
                continue
            elif command == "accessibility_checker":
                # Check for alt-text on all images (Criterion 3.1.3)
                if BeautifulSoup is None:
                    print(f"❌ BeautifulSoup not installed. Install beautifulsoup4 and lxml")
                    notebook_results[notebook] = "failure"
                    result = 1
                    continue

                try:
                    with open(notebook, 'r', encoding='utf-8') as f:
                        nb_data = json.load(f)
                except Exception as e:
                    print(f"❌ Error reading notebook {notebook}: {e}")
                    notebook_results[notebook] = "failure"
                    result = 1
                    continue

                issues = []

                for cell_idx, cell in enumerate(nb_data.get('cells', [])):
                    # Check markdown cells for images
                    if cell.get('cell_type') == 'markdown':
                        source = cell.get('source', [])
                        if isinstance(source, list):
                            source = ''.join(source)

                        # Check HTML img tags
                        soup = BeautifulSoup(source, 'html.parser')
                        for img in soup.find_all('img'):
                            alt = img.get('alt', '').strip()
                            if not alt:
                                issues.append(f"Cell {cell_idx}: <img> tag missing alt text")

                        # Check markdown image syntax: ![alt](url)
                        md_images = re.findall(r'!\[(.*?)\]\(.*?\)', source)
                        for idx, alt_text in enumerate(md_images):
                            if not alt_text.strip():
                                issues.append(f"Cell {cell_idx}: Markdown image missing alt text")

                    # Check output cells for figures
                    elif cell.get('cell_type') == 'code':
                        outputs = cell.get('outputs', [])
                        for output in outputs:
                            if output.get('output_type') in ['display_data', 'execute_result']:
                                # Check for image outputs
                                data = output.get('data', {})
                                if 'image/png' in data or 'image/jpeg' in data or 'image/jpg' in data:
                                    metadata = output.get('metadata', {})
                                    # Check for alt text in metadata
                                    if not metadata.get('alt_text') and not metadata.get('alt'):
                                        issues.append(f"Cell {cell_idx}: Figure output missing alt text metadata")

                # Report results
                if not issues:
                    print(f"✅ All images have alt-text in {notebook}")
                    notebook_results[notebook] = "success"
                else:
                    print(f"❌ Found {len(issues)} accessibility issue(s) in {notebook}:")
                    for issue in issues:
                        print(f"   - {issue}")
                    print(f"\nAdd alt-text to all images for screen reader compatibility")
                    notebook_results[notebook] = "failure"
                    result = 1

                # Skip subprocess call for accessibility_checker
                continue

            try:
                subprocess.run(cmd, check=True)

                # Special handling for pynblint
                if command == "pynblint" and Path(lint_file).exists():
                    with open(lint_file, 'r') as f:
                        lint_data = json.load(f)
                    if any('recommendation' in lint for lint in lint_data.get('lints', [])):
                        print(f"❌ pynblint failed for {notebook}")
                        notebook_results[notebook] = "failure"
                        result = 1
                    else:
                        print(f"✅ {command} finished successfully for {notebook}")
                        notebook_results[notebook] = "success"
                else:
                    print(f"✅ {command} finished successfully for {notebook}")
                    notebook_results[notebook] = "success"
            except subprocess.CalledProcessError:
                print(f"❌ {command} failed for {notebook}")
                notebook_results[notebook] = "failure"
                result = 1
            except Exception as e:
                print(f"❌ Error processing {notebook}: {e}")
                notebook_results[notebook] = "failure"
                result = 1

        # Write results to JSON file
        output_dir = os.getenv("QA_OUTPUT_DIR", "qa_outputs")
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        results_file = Path(output_dir) / f"{command}-results.json"

        results_data = {
            "command": command,
            "results": notebook_results
        }

        with open(results_file, 'w') as f:
            json.dump(results_data, f, indent=2)

        print(f"\nResults written to {results_file}")

        sys.exit(result)
        EOF
